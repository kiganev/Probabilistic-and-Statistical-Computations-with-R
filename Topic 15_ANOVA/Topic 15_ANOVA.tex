\documentclass[10pt]{beamer}
\usetheme{CambridgeUS}
%\usetheme{Boadilla}
\definecolor{myred}{RGB}{163,0,0}
%\usecolortheme[named=blue]{structure}
\usecolortheme{dove}
\usefonttheme[]{professionalfonts}
\usepackage[english]{babel}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{mathtools}
\usepackage{commath}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest,compat/show suggested version=false}
\usetikzlibrary{arrows,shapes,calc,backgrounds}
\usepackage{bm}
\usepackage{textcomp}
%\usepackage{gensymb}
%\usepackage{verbatim}
\usepackage{mathrsfs}  
\usepackage{paratype}
\usepackage{mathpazo}
\usepackage{listings}
\usepackage{csvsimple}
\usepackage{booktabs}

\newcommand{\cc}[1]{\texttt{\textcolor{blue}{#1}}}

\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\E}{\mathsf{E}}
\DeclareMathOperator{\var}{\mathsf{Var}}
\DeclareMathOperator{\cov}{\mathsf{Cov}}
\DeclareMathOperator{\corr}{\mathsf{Corr}}
\DeclareMathOperator{\plim}{plim}
\DeclareMathOperator{\rank}{rank}


\definecolor{ttcolor}{RGB}{0,0,1}%{RGB}{163,0,0}
\definecolor{mygray}{RGB}{248,249,250}

% Number theorem environments
\setbeamertemplate{theorem}[ams style]
\setbeamertemplate{theorems}[numbered]

% Reset theorem-like environments so that each is numbered separately
\usepackage{etoolbox}
\undef{\definition}
\theoremstyle{definition}
\newtheorem{definition}{\translate{Definition}}

% Change colours for theorem-like environments
\definecolor{mygreen1}{RGB}{0,96,0}
\definecolor{mygreen2}{RGB}{229,239,229}
\setbeamercolor{block title}{fg=white,bg=mygreen1}
\setbeamercolor{block body}{fg=black,bg=mygreen2}

\lstdefinestyle{numbers}{numbers=left, stepnumber=1, numberstyle=\tiny, numbersep=10pt}
\lstdefinestyle{MyFrame}{backgroundcolor=\color{yellow},frame=shadowbox}

\lstdefinestyle{rstyle}%
{language=R,
	basicstyle=\footnotesize\ttfamily,
	backgroundcolor = \color{mygray},
	commentstyle=\slshape\color{green!50!black},
	keywordstyle=\color{blue},
	identifierstyle=\color{blue},
	stringstyle=\color{orange},
	%escapechar=\#,
	rulecolor = \color{mygray}, 
	showstringspaces = false,
	showtabs = false,
	tabsize = 2,
	emphstyle=\color{red},
	frame = single}

\lstset{language=R,frame=single}    

\hypersetup{colorlinks, urlcolor=blue, linkcolor = myred}

\AtBeginSection{\frame{\sectionpage}}

% Remove Section 1, Section 2, etc. as titles in section pages
\defbeamertemplate{section page}{mine}[1][]{%
	\begin{centering}
		{\usebeamerfont{section name}\usebeamercolor[fg]{section name}#1}
		\vskip1em\par
		\begin{beamercolorbox}[sep=12pt,center]{part title}
			\usebeamerfont{section title}\insertsection\par
		\end{beamercolorbox}
	\end{centering}
} 

\setbeamertemplate{section page}[mine] 

\beamertemplatenavigationsymbolsempty


\title{R403: Probabilistic and Statistical Computations\\ with R}
\subtitle{Lecture 15: \textcolor{myred}{Analysis of Variance (ANOVA)}}
\author{Kaloyan Ganev}

\date{2022/2023} 

\begin{document}
\maketitle

\begin{frame}[fragile]
\frametitle{Lecture Contents}
\tableofcontents
\end{frame}

\section{Introduction}
\begin{frame}[fragile]
\frametitle{What is ANOVA?}
\begin{itemize}
	\item Developed by Ronald Fisher (\url{https://en.wikipedia.org/wiki/Ronald_Fisher})
	\item Decrypted as ``Analysis of Variance''
	\item One of the most frequently used statistical techniques in empirical work
	\item Assumes linear relationships among variables
	\item Not a single model but rather a collection (a class) of models
	\item The basic idea consists in attempting to partition variation into components corresponding to different sources of that variation
	\item Yet, at the same time, its purpose is not to analyse variances but to analyse \textbf{variation in means}
	\item In other words, it is used to test differences in means for statistical significance
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Why ANOVA?}
\begin{itemize}
	\item The method is employed in  scientific studies in the analysis of data (experimental, observational, or mixed)
	\item Allows to identify the presence or absence of statistically significant effects from the presence of experimental treatments or observational factors
	\item Allows to identify effects of a variable after controlling for other variables' influence
	\item Simple and robustly designed technique
\end{itemize} 
\end{frame}

\section{A Quick Review of Quadratic Forms}
\begin{frame}[fragile]
\frametitle{Quadratic Forms Defined}
\begin{definition}
	A \textbf{homogeneous polynomial} is a polynomial whose non-zero terms are of one and the same degree.
\end{definition}

\begin{definition}
	A \textbf{quadratic form} is a homogeneous polynomial of degree 2 in $n$ variables.
\end{definition}

\begin{definition}
	A \textbf{real quadratic form} is a quadratic form whose variables and coefficients are all real.
\end{definition}

We will deal exclusively with real quadratic forms.
\end{frame}

\begin{frame}[fragile]
\frametitle{Examples of Quadratic Forms}
\begin{itemize}
	\item The following is a quadratic form in $X_{1},X_{2}$ and $X_{3}$:
	\[
		X_{1}^{2} + X_{2}^{2} + X_{3}^{2} - 2X_{1}X_{2}
	\]
	\item The following is NOT a quadratic form in $X_{1}$ and $X_{2}$ (\textcolor{red}{why?}):
	\[
		X_{1}^{2} + X_{2}^{2} - 2X_{1} - 4X_{2} + 5
	\]
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Sample Variance as A Quadratic Form}
\begin{itemize}
	\item Let $\overline{X}$ and $S^{2}$ be respectively the sample mean and the sample variance of an arbitrary distribution
	\item We know that:
	\[
		S^{2} = \frac{1}{n-1}\sum_{i=1}^{n}(X_{i} - \overline{X})^{2}
	\]
	where $n$ is sample size
	\item This can also be written as follows:
	\[
		(n-1)S^{2} = \sum_{i=1}^{n}(X_{i} - \overline{X})^{2}
	\]
	\item The sample mean equals:
	\[
		\overline{X} = \frac{X_{1} + X_{2} + \ldots + X_{n}}{n}
	\]
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Sample Variance as A Quadratic Form (2)}
\begin{itemize}
	\item Plug this expression into the right-hand side of the variance expression to get:
	\[
		(n-1)S^{2} = \sum_{i=1}^{n}\left(X_{i} - \frac{X_{1} + X_{2} + \ldots + X_{n}}{n}\right)^{2}
	\]
	\item Expand the stuff in the parentheses:
	\[
		(n-1)S^{2} = \frac{n-1}{n}\sum_{i=1}^{n}X_{i}^{2} - \frac{1}{n}\sum_{i\neq j}X_{i}X_{j}
	\]
	\item Obviously, this is a quadratic form in $X_{1}, X_{2}, \ldots, X_{n}$
	
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Sample Variance as A Quadratic Form (3)}
\begin{itemize}
	\item Note that if the sample is drawn from a $N(\mu, \sigma^{2})$ distribution, then:
	\[
		\frac{(n-1)S^{2}}{\sigma^{2}} \sim \chi^{2}(n-1)
	\]
	\item Note also that this result is independent from the value of $\mu$
	\item This, for example, allows to construct confidence intervals for $\sigma^{2}$ when $\mu$ is unknown
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The Distribution of Certain Quadratic Forms in n.i.d. Random Variables}

\begin{theorem}
	Let $Q_{1}, Q_{2}, \ldots, Q_{k}, Q$ be $(k+1)$ real quadratic forms in $n$ random variables (each random variable being n.i.d. $(\mu,\sigma^{2})$) where:
	\[
		Q = Q_{1} + Q_{2} + \ldots + Q_{k-1} + Q_{k}
	\]
	If $\displaystyle \frac{Q}{\sigma^{2}}\sim \chi^{2}(r),\,\frac{Q_{1}}{\sigma^{2}}\sim \chi^{2}(r_{1}),\ldots, \frac{Q_{k-1}}{\sigma^{2}}\sim \chi^{2}(r-1)$ and $Q_{k} \geq 0$, then:
	
	\quad\\
	\begin{itemize}
		\item[(a)] $Q_{1}, Q_{2}, \ldots, Q_{k}$ are independent, from which also follows:
		\item[(b)] $\displaystyle \frac{Q_{k}}{\sigma^{2}}\sim \chi^{2}(r_{k})$, where $r_{k} = r - (r_{1} + \ldots + r_{k-1})$
	\end{itemize}
\end{theorem}
\end{frame}

\begin{frame}[fragile]
\frametitle{The Distribution of Certain Quadratic Forms in n.i.d. Random Variables (2)}
\begin{itemize}
	\item Let $X\sim N(\mu, \sigma^{2})$
	\item Draw a sample of size $n = ab$ from the above distribution
	\item Each drawing is independent and produces itself a random variable with mean $\mu$ and variance $\sigma^{2}$
	\item If we arrange the observations in $a$ rows and $b$ columns, the arrangement would look like:
	\[
		\begin{matrix}
			X_{11} & X_{12} & \cdots & X_{1b}\\
			X_{21} & X_{22} & \cdots & X_{2b}\\
			\cdots & \cdots & \cdots & \cdots\\
			X_{a1} & X_{12} & \cdots & X_{ab}\\
		\end{matrix}			
	\]
	\item With this, we could think of having $a$ samples (by rows) or $b$ samples (by columns)
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The Distribution of Certain Quadratic Forms in n.i.d. Random Variables (3)}
We define now some statistics:
\begin{itemize}
	\item Overall (grand) mean:
	\[
		\overline{X}_{\cdot\cdot} = \frac{1}{ab}\sum_{i=1}^{a}\sum_{j=1}^{b}X_{ij}
	\]
	\item Row means:
	\[
		\overline{X}_{i\cdot} = \frac{1}{b}\sum_{j=1}^{b}X_{ij},\quad i = 1,2,\ldots, a
	\]
	\item Column means:
	\[
		\overline{X}_{\cdot b} = \frac{1}{a} \sum_{i=1}^{a}X_{ij}, \quad j = 1,2,\ldots,b
	\]
	
	(In total: $a + b + 1$)
	
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The Distribution of Certain Quadratic Forms in n.i.d. Random Variables (4)}
\begin{itemize}
	\item We will consider an example
	\item We have a sample of size $n = ab$; the sample variance is $S^{2}$
	\item Using the sample variance formula, we can write:
	\[
		(ab-1)S^{2} = \sum_{i=1}^{a}\sum_{j=1}^{b}(X_{ij} - \overline{X}_{\cdot\cdot})^{2}
	\]
	\item Add and subtract $\overline{X}_{i\cdot}$ in the parentheses:
\end{itemize}
	\[
	\begin{array}{lcl}
		(ab-1)S^{2} & = & \displaystyle \sum_{i=1}^{a}\sum_{j=1}^{b}[(X_{ij} - \overline{X}_{i\cdot}) + (\overline{X}_{i\cdot} - \overline{X}_{\cdot\cdot})]^{2}
	\end{array}
	\]
\end{frame}

\begin{frame}[fragile]
\frametitle{The Distribution of Certain Quadratic Forms in n.i.d. Random Variables (5)}
\begin{itemize}
	\item Then expand to get:
	\[
	\begin{array}{lcl}
		(ab-1)S^{2} & = & \displaystyle \sum_{i=1}^{a}\sum_{j=1}^{b}(X_{ij} - \overline{X}_{i\cdot})^{2} + \sum_{i=1}^{a}\sum_{j=1}^{b}(\overline{X}_{i\cdot} - \overline{X}_{\cdot\cdot})^{2},
	\end{array}
	\]
	
	where we made use of the fact that $\displaystyle 2\sum_{i=1}^{a}\sum_{j=1}^{b}(X_{ij} - \overline{X}_{i\cdot})(\overline{X}_{i\cdot} - \overline{X}_{\cdot\cdot}) = 0$.
	\item Note that there is no $j$ index in the parentheses in the far-right-hand-side double sum; therefore:
	\[
	\begin{array}{lcl}
		(ab-1)S^{2} & = & \displaystyle \sum_{i=1}^{a}\sum_{j=1}^{b}(X_{ij} - \overline{X}_{i\cdot})^{2} + b\sum_{i=1}^{a}(\overline{X}_{i\cdot} - \overline{X}_{\cdot\cdot})^{2}
	\end{array}
	\]
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The Distribution of Certain Quadratic Forms in n.i.d. Random Variables (6)}
\begin{itemize}
	\item All those are quadratic forms, and we can write the last expression on the previous slide more briefly as follows:
	\[
		Q = Q_{1} + Q_{2}
	\]
	\item First, we will apply Theorem 1 to show that $Q_{1}$ and $Q_{2}$ are independent
	\item We know that $\displaystyle Q = \frac{(ab-1)S^{2}}{\sigma^{2}} \sim \chi^{2}(ab-1)$
	\item Now consider $Q_{1}$. We can write it also in the following way:
	\[
		Q_{1} = \sum_{i=1}^{a}\left[(b-1)\left(\frac{1}{b-1}\sum_{j=1}^{b}(X_{ij} - \overline{X}_{i\cdot})^{2}\right)\right]
	\]
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The Distribution of Certain Quadratic Forms in n.i.d. Random Variables (7)}
\begin{itemize}
	\item For each value of $i$, the expression in the big parentheses is the sample variance of a sample of size $b$
	\item The ratio:
	\[
		\frac{\displaystyle \frac{1}{b-1}\sum_{j=1}^{b}(X_{ij} - \overline{X}_{i\cdot})^{2}}{\sigma^{2}}
	\]
	has therefore $\chi^{2}(b-1)$ distribution
	\item $\displaystyle \frac{Q_{1}}{\sigma^{2}}$ is practically the sum $a$ such ratios (multiplied by $(b-1)$)
	\item Therefore:
	\[
		\frac{Q_{1}}{\sigma^{2}} \sim \chi^{2}(a(b-1))
	\]
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{The Distribution of Certain Quadratic Forms in n.i.d. Random Variables (7)}
\begin{itemize}
	\item It is obvious that $Q_{2} \geq 0$ since it is a sum of squares multiplied by a positive number $b$
	\item From Theorem 1 follows that $Q_{1}$ and $Q_{2}$ are independent and $\displaystyle \frac{Q_{2}}{\sigma^{2}} \sim \chi^{2}(a-1)$
	\item The ratios of those $\chi^{2}$ statistics form $F$ statistics which can be used to test some interesting statistical hypotheses
	\item \textcolor{red}{For an exercise, try the same by replacing $X_{ij} - \overline{X}_{\cdot\cdot}$ by $(X_{ij} - \overline{X}_{\cdot j}) + (\overline{X}_{\cdot j} -  \overline{X}_{\cdot\cdot})$}
\end{itemize}
\end{frame}

\section{One-Way ANOVA}
\begin{frame}[fragile]
\frametitle{One-Way ANOVA}
\begin{itemize}
	\item In the beginning we mentioned briefly that ANOVA is a method of comparing the means of several populations
	\item Often those populations are assumed to be normally distributed
	\item However, in addition to that, ANOVA allows point and interval estimation 
	\item In this context, inference is based on the so-called \textbf{contrasts}
	\item We will make an introduction to this matter in what follows
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{One-Way ANOVA (2)}
\begin{itemize}
	\item Consider the following independent random variables:
	\[
		X_{j} \sim N(\mu_{j},\sigma^{2}),\quad j = 1,2,\ldots,b,
	\]
	where all $\mu_{j}$ are unknown but $\sigma^{2}$ is known and common to all variables (homoscedasticity)
	\item Let $X_{1j},X_{2j},\ldots,X_{aj},\,j=1,2,\ldots,b$ be random samples of size $a$ from each variable
	\item Assume that each observation (the data) $X_{ij}$ in the $b$ samples is most appropriately described by the following model:
	\[
		X_{ij} = \mu_{j} + e_{ij}, \quad i = 1,2,\ldots,a,\quad j = 1,2,\ldots,b,
	\]
	where $e_{ij}\sim N(0,\sigma^{2})$
	
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{One-Way ANOVA (3)}
\begin{itemize}
	\item From the practical perspective, the $b$ samples can be thought of as groups undergoing treatments (e.g. with different medicines in clinical trials)
	\item In the language of ANOVA, this is a case where we have one factor (in this case medication applied) at $b$ levels
	\item Therefore, the model is called a \textbf{one-way model} (the effects of a single factor only are studied)
	\item The mathematical formulation of the model is interpreted as follows: the outcomes $X_{ij}$ are the result of systematic causes ($\mu_{j}$) and random causes ($e_{ij}$)
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{One-Way ANOVA (4)}
\begin{itemize}
	\item We are virtually not able to separate the two types of influences
	\item However, by using multiple samples (in our case $b$ samples), after comparing the means we are able to tell whether a specific treatment is effective
	\item The comparison is based on statistical methods, i.e. deciding whether a treatment is effective is based on tests of significance
	\item \textcolor{red}{Note that based on the above-said, ANOVA is analogical to $t$-tests; the latter, however, cannot be used when more than two groups of data are compared!}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{One-Way ANOVA (5)}
\begin{itemize}
	\item The classical ANOVA hypothesis:
	\[
		H_{0}:\quad \mu_{1} = \mu_{2} = \ldots = \mu_{b} = \mu,
	\]
	
	where $\mu$ is unspecified, tested against a general alternative $H_{1}$ (``at least one mean is different'')
	\item Let $X_{j}$ be the response from the $j$th treatment, and let $\mu_{j} = \mathsf{E}(X_{j})$
	\item If $X_{j}\sim N(\mu_{j},\sigma^{2})$, then $H_{0}$ states that all treatments have the same effect
	\item A likelihood ratio test is used to test the validity of $H_{0}$
	\item The aim of the test is to compare the ratio of variances with the one present when means differ only due to random influences
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{One-Way ANOVA (6)}
\begin{itemize}
	\item Using the formula for the multivariate normal density, the first likelihood function describing the case when all means are equal is:
	\[
		L(\omega) = \left(\frac{1}{\sqrt{2\pi\sigma^{2}}}\right)^{ab}\exp\left[-\frac{1}{2\sigma}\sum_{j=1}^{b}\sum_{i=1}^{a}(x_{ij} - \mu)^{2}\right]
	\]
	where $\omega = \{(\mu_{1},\mu_{2},\ldots,\mu_{b},\sigma^{2}): -\infty < \mu_{1} = \ldots = \mu_{b} = \mu < \infty, 0 < \sigma^{2} < \infty\}$
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{One-Way ANOVA (7)}
\begin{itemize}
	\item The values of $\mu$ and $\sigma^{2}$ that maximize this function are respectively:
	\[
		\overline{x}_{\cdot\cdot} = \frac{1}{ab}\sum_{j=1}^{b}\sum_{i=1}^{a}x_{ij}
	\]
	
	\[
		v = \frac{1}{ab}\sum_{j=1}^{b}\sum_{i=1}^{a}(x_{ij} - \overline{x}_{\cdot\cdot})^{2}
	\]
	and the maximum of the function equals:	
	\[
		L(\widehat{\omega}) = \left[\sqrt{\frac{ab}{2\pi\sum_{i=1}^{a}(x_{ij} - \overline{x}_{\cdot\cdot})^{2}}}\right]^{ab}\exp\left(-\frac{ab}{2}\right)
	\]
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{One-Way ANOVA (8)}
\begin{itemize}
	\item The second likelihood function relates to the case when means are not equal:
	\[
		L(\Omega) = \left(\frac{1}{\sqrt{2\pi\sigma^{2}}}\right)^{ab}\exp\left[-\frac{1}{2\sigma}\sum_{j=1}^{b}\sum_{i=1}^{a}(x_{ij} - \mu_{j})^{2}\right]
	\]
	where $\Omega = \{(\mu_{1},\mu_{2},\ldots,\mu_{b},\sigma^{2}): -\infty < \mu_{j} < \infty, 0 < \sigma^{2} < \infty\}$ 
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{One-Way ANOVA (9)}
\begin{itemize}
	\item The values of $\mu$ and $\sigma^{2}$ that maximize this function are respectively:
	\[
		\overline{x}_{\cdot j} = \frac{1}{a}\sum_{i=1}^{a}x_{ij}
	\]
	
	\[
		w = \frac{1}{ab}\sum_{j=1}^{b}\sum_{i=1}^{a}(x_{ij} - \overline{x}_{\cdot j})^{2}
	\]
	and the maximum of the function equals:	
	\[
		L(\widehat{\Omega}) = \left[\sqrt{\frac{ab}{2\pi\sum_{i=1}^{a}(x_{ij} - \overline{x}_{\cdot j})^{2}}}\right]^{ab}\exp\left(-\frac{ab}{2}\right)
	\]
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{One-Way ANOVA (10)}
\begin{itemize}
	\item Take the likelihood ratio:
	\[
		\Lambda = \frac{L(\widehat{\omega})}{L(\widehat{\Omega})} = \left[\sqrt{\frac{\displaystyle \sum_{i=1}^{a}\sum_{j=1}^{b}(x_{ij} - \overline{x}_{\cdot j})^{2}}{\displaystyle \sum_{i=1}^{a}\sum_{j=1}^{b}(x_{ij} - \overline{x}_{\cdot \cdot})^{2}}}\right]^{ab}
	\]
	\item Recall from the quadratic forms example that $\displaystyle Q =  \sum_{j=1}^{b}\sum_{i=1}^{a}(X_{ij} - \overline{X}_{\cdot\cdot})^{2}$. From this follows that the statistic:
	\[
		V = \frac{1}{ab}\sum_{j=1}^{b}\sum_{i=1}^{a}(X_{ij} - \overline{X}_{\cdot\cdot})^{2} = \frac{Q}{ab}
	\]
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{One-Way ANOVA (11)}
\begin{itemize}
	\item If (and when) you do the exercise on quadratic forms, you get to the following expressions:
	\[
		Q_{3} = \sum_{j=1}^{b}\sum_{i=1}^{a}(X_{ij} - \overline{X}_{\cdot j})^{2}
	\]
	\[
		Q_{4} = a\sum_{j=1}^{b}(\overline{X}_{\cdot j} - \overline{X}_{\cdot \cdot})^{2}
	\]
	\item By the theorem we also find out that $Q_{3}$ and $Q_{4}$ are independent, and:
	\[
		\frac{Q_{3}}{\sigma^{2}} \sim \chi^{2}(b(a-1)), \quad \frac{Q_{4}}{\sigma^{2}} \sim \chi^{2}(b-1)
	\]
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{One-Way ANOVA (12)}
\begin{itemize}
	\item Thus $\Lambda$ can be viewed as:
	\[
		\Lambda = \left(\sqrt{\frac{Q_{3}}{Q}}\right)^{ab}
	\]
	or:
	\[
		\Lambda^{2/ab} = \frac{Q_{3}}{Q}
	\]
	\item Because $Q = Q_{3} + Q_{4}$,
	\[
		\Lambda^{2/ab} = \frac{Q_{3}}{Q_{3} + Q_{4}} = \frac{1}{1 + Q_{4}/Q_{3}}
	\]
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{One-Way ANOVA (13)}
\begin{itemize}
	\item The significance level that corresponds to the test of $H_{0}$ is:
	\[
		\alpha = P_{H_{0}}\left[\frac{1}{1 + Q_{4}/Q_{3}} \leq \lambda_{0}^{2/ab}\right] = P_{H_{0}}\left[Q_{4}/Q_{3} \geq \lambda_{0}^{-2/ab} - 1\right]
	\]
	\item Multiply both sides of the inequality by $\displaystyle \frac{b(a-1)}{b-1}$ (a positive number) and denote $\displaystyle c = \displaystyle \frac{b(a-1)}{b-1}(\lambda_{0}^{-2/ab} - 1)$ to get:
	\[
		\alpha = P_{H_{0}}\left[\frac{Q_{4}/(b-1)}{Q_{3}/[b(a-1)]} \geq c\right]
	\]
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{One-Way ANOVA (14)}
\begin{itemize}
	\item But at the same time we know that:
	\[
		F = \frac{Q_{4}/[\sigma^{2}(b-1)]}{Q_{3}/[\sigma^{2}b(a-1)]} = \frac{Q_{4}/[(b-1)]}{Q_{3}/[b(a-1)]} \sim F_{b-1, b(a-1)}
	\]
	\item Therefore, $H_{0}$ can be tested with an $F$-statistic
	\item The critical point is chosen from the $F$-table -- the one that corresponds to the specified significance level and the respective degrees of freedom
	\item \textcolor{orange}{Note:} Testing means equality does not require that sample size to be equal across the $b$ samples
\end{itemize}
\end{frame}

\section{ANOVA in R}
\begin{frame}[fragile]
\frametitle{An Example of One-Way ANOVA in R}
\begin{itemize}
	\item Start with the following dataset: \texttt{three\_stores.csv}\footnote{Data taken from Hanke and Reitsch (1991), p. 405.}
	\item Explanation of data: three stores, for each store the values of six purchases are randomly sampled
	\item Question: Do the three stores have the same average amount per purchase?
	\item Load the data in R and calculate averages:
	\begin{lstlisting}[style = rstyle, breaklines]
	avgbuy <- read.csv("three_stores.csv")
	attach(avgbuy)
	colMeans(avgbuy) # Means per store
	mean(unlist(avgbuy)) # Grand mean for all stores
	boxplot(avgbuy, col = c("red","green","blue"))
	\end{lstlisting}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{An Example of One-Way ANOVA in R (2)}
\centerline{\includegraphics[scale=0.45]{./data/fig1}}
\end{frame}


\begin{frame}[fragile]
\frametitle{An Example of One-Way ANOVA in R (3)}
\begin{itemize}
	\item Null hypothesis: the three populations (stores) have equal means
	\item Looks like the first two means are approximately equal but the third is not; the question is whether this inequality is due to chance only or not
	\item Before running ANOVA, the data should be stacked:
	\begin{lstlisting}[style = rstyle, breaklines]
	avgbuys <- stack(avgbuy)
	names(avgbuys)
	\end{lstlisting}
	\item With the above names, now run the ANOVA:
	\begin{lstlisting}[style = rstyle, breaklines]
	myanova <- aov(formula = values ~ ind, data = avgbuys)
	myanova
	summary(myanova)
	\end{lstlisting}
\end{itemize}
\end{frame}

\section{Two-Way ANOVA}
\begin{frame}[fragile]
\frametitle{Two-Way ANOVA}
\begin{itemize}
	\item In one-way ANOVA we dealt with one factor at $b$ levels
	\item Now, let there be two factors, $A$ and $B$, respectively having $a$ and $b$ levels
	\item In such a setting, we have \textbf{two-way (two-factor) ANOVA}
	\item Let $X_{ij}\sim n.i.d (\mu_{ij},\sigma^{2}),\, i = 1, 2, \ldots, a,\,\, j = 1, 2, \ldots, b$ the values of the responses when $A$ is at level $i$ and $B$ is at level $j$
	\item The total number of levels pairs is $n = ab$ (each pair is actually a treatment)
	\item The mean $\mu_{ij}$ is interpreted as the mean response to a treatment 
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Two-Way ANOVA (2)}
\begin{itemize}
	\item Denote:
	\[
		\overline{\mu} = \frac{1}{ab}\sum_{i=1}^{a}\sum_{j=1}^{b}\mu_{ij},\quad \overline{\mu}_{i\cdot} = \frac{1}{b}\sum_{j=1}^{b}\mu_{ij},\quad \overline{\mu}_{\cdot j} = \frac{1}{a}\sum_{i=1}^{a}\mu_{ij}
	\]
	where  $i = 1, 2, \ldots, a,\,\, j = 1, 2, \ldots, b$
	\item These are respectively the grand mean, the row means, and the column means (if we arrange factor levels in a table)
	\item Consider the following additive model:
	\[
		\mu_{ij} = \overline{\mu} + (\overline{\mu}_{i\cdot} - \overline{\mu}) + (\overline{\mu}_{\cdot j} - \overline{\mu})
	\]
	\item It is interpreted as follows: the mean $\mu_{ij}$ is the result of the additive main effect\footnote{Besides main effects, there could also be interaction effects which are non-additive.} of level $i$ of factor $A$ and the additive main effect of level $j$ of factor $B$
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Two-Way ANOVA (3)}
\begin{itemize}
	\item For simplicity, denote:
	\[
		\mu = \overline{\mu},\quad \alpha_{i} = \overline{\mu}_{i\cdot} - \overline{\mu},\quad \beta_{j} = \overline{\mu}_{\cdot j} - \overline{\mu}
	\]
	\item Then the model becomes:
	\[
		\mu_{ij} = \mu + \alpha_{i} + \beta_{j}
	\]
	where $\displaystyle \sum_{i=1}^{a}\alpha_{i} = 0$ and $\displaystyle \sum_{j=1}^{b}\beta_{j} = 0$
	\item In empirical data this relationship is not exactly fulfilled
	\item If we add random disturbances, we get:
	\[
		\mu_{ij} = \mu + \alpha_{i} + \beta_{j} + \varepsilon_{ij}
	\]
	\item This is referred to as the \textbf{two-way ANOVA model} 
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Two-Way ANOVA Setup: A Numerical Example}
\begin{itemize}
	\item Taken from Kutner et. al. (2005), p. 817
	\item Consider the following table of mean learning times (in minutes):
	\bigskip\\
	\begin{tabular}{l|ccc|c}
		\toprule
		& \multicolumn{3}{c|}{\textbf{Factor $B$: Age}}\\
		\cline{2-4}
		\textbf{Factor $A$: Gender} & \textbf{Young} & \textbf{Middle} & \textbf{Old} & \textbf{Row average}\\ 
		\midrule
		Male & 9 & 11 & 16 & 12\\
		Female & 9 & 11 & 16 & 12\\
		\midrule
		\textbf{Column average} & 9 & 11 & 16 & 12\\
		\bottomrule
	\end{tabular} 
	\bigskip\\
	\item In this case we have $a=2$ and $b=3$
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Two-Way ANOVA Setup: A Numerical Example (2)}
\begin{itemize}
	\item The main gender effects (in minutes) are:
	\[
		\begin{array}{lcl}
			\alpha_{1} = 12 - 12 = 0\\
			\alpha_{2} = 12 - 12 = 0
		\end{array}
	\]
	\item From this it turns out that gender has no effect on average learning times
	\item The main age effects (in minutes) are:
	\[
		\begin{array}{lcl}
			\beta_{1} = 9 - 12 = -3\\
			\beta_{2} = 11 - 12 = -1\\
			\beta_{3} = 16 - 12 = 4
		\end{array}
	\]
	\item This shows that mean learning time increases with age, i.e. age has an effect
	\item (This model could however be reduced to a one-way ANOVA)
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Two-Way ANOVA Setup: A Numerical Example (3)}
\begin{itemize}
	\item It is easy to see that all mean responses can be obtained using the model equation
	\item For example:
	\[
		\begin{array}{lcl}
			\mu_{21} = 12 + 0 - 3 = 9\\
			\mu_{13} = 12 + 0 + 4 = 16
		\end{array}
	\]
	\item We also note that for additive models, when we plot $\mu_{ij}$ against $j$, we get parallel lines
	\item Such plots are called \textbf{treatment mean plots}, \textbf{mean profile plots}, or \textbf{interaction plots}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Two-Way ANOVA Setup: A Numerical Example (4)}
\centerline{\includegraphics[scale=0.45]{./data/fig2}}
\end{frame}

\begin{frame}[fragile]
\frametitle{Two-Way ANOVA Setup: A Second Example}
\begin{itemize}
	\item Same as the previous, changed values of $\mu_{ij}$
	\bigskip\\
	\begin{tabular}{l|ccc|c}
		\toprule
		& \multicolumn{3}{c|}{\textbf{Factor $B$: Age}}\\
		\cline{2-4}
		\textbf{Factor $A$: Gender} & \textbf{Young} & \textbf{Middle} & \textbf{Old} & \textbf{Row average}\\ 
		\midrule
		Male & 11 & 13 & 18 & 14\\
		Female & 7 & 9 & 14 & 10\\
		\midrule
		\textbf{Column average} & 9 & 11 & 16 & 12\\
		\bottomrule
	\end{tabular}
	\bigskip\\
	\item The main gender effects (in minutes) are:
	\[
		\begin{array}{lcl}
			\alpha_{1} = 14 - 12 = 2\\
			\alpha_{2} = 10 - 12 = -2
		\end{array}
	\]
	\item This time gender has an effect on average learning times
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Two-Way ANOVA Setup: A Second Example (2)}
\centerline{\includegraphics[scale=0.45]{./data/fig3}}
\end{frame}

\begin{frame}[fragile]
\frametitle{Two-Way ANOVA Setup: A Second Example (3)}
\begin{itemize}
	\item The main age effects (in minutes) are:
	\[
		\begin{array}{lcl}
			\beta_{1} = 9 - 12 = -3\\
			\beta_{2} = 11 - 12 = -1\\
			\beta_{3} = 16 - 12 = 4
		\end{array}
	\]
	\item In this case also mean learning time increases with age, i.e. age has an effect
	\item The graph differs, however
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Two-Way ANOVA Setup: A Second Example (4)}
\centerline{\includegraphics[scale=0.45]{./data/fig4}}
\end{frame}

\begin{frame}[fragile]
\frametitle{Two-Way ANOVA: The Main Effects Hypotheses}
\begin{itemize}
	\item Having in mind the two examples and the knowledge we have on one-way ANOVA, we can now state the relevant hypotheses:
	\begin{block}{Main Effects Hypotheses}
		\[
			\begin{array}{lcl}
				H_{0,A}: \, \alpha_{1} = \ldots = \alpha_{a} = 0\\
				H_{1,A}: \, \textsf{at least one } \alpha \neq 0\\
				\quad\\
				H_{0,B}: \, \beta_{1} = \ldots = \beta_{b} = 0\\
				H_{1,B}: \, \textsf{at least one } \beta \neq 0
			\end{array}
		\]
	\end{block}
\end{itemize}
\end{frame}



\section{Additional Readings}
\begin{frame}[fragile]
\frametitle{Additional Readings}
\begin{itemize}
	\item Hogg, R., J. McKean and A. Craig (2013): \emph{Introduction to Mathematical Statistics}, Pearson, 7th edn.
	\item Kutner, M., C. Nachtsheim, J. Neter and W. Li (2005): \emph{Applied Linear Statistical Models}, McGraw-Hill Irwin, 5th edn.
	\item Tabachnick, B. and L. Fidell (2013): \emph{Using Multivariate Statistics}, Pearson, 6th edn.
\end{itemize}
\end{frame}

\end{document}

Books:
Tabachnick
Casella and Berger
Kutner
