\documentclass[10pt]{beamer}
\usetheme{CambridgeUS}
%\usetheme{Boadilla}
\definecolor{myred}{RGB}{163,0,0}
%\usecolortheme[named=blue]{structure}
\usecolortheme{dove}
\usefonttheme[]{professionalfonts}
\usepackage[english]{babel}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{mathtools}
\usepackage{commath}
\usepackage{xcolor}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=newest,compat/show suggested version=false}
\usetikzlibrary{arrows,shapes,calc,backgrounds}
\usepackage{bm}
\usepackage{textcomp}
%\usepackage{gensymb}
%\usepackage{verbatim}
\usepackage{mathrsfs}  
\usepackage{paratype}
\usepackage{mathpazo}
\usepackage{listings}
\usepackage{csvsimple}
\usepackage{booktabs}

\newcommand{\cc}[1]{\texttt{\textcolor{blue}{#1}}}

\DeclareMathOperator{\adj}{adj}
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\E}{\mathsf{E}}
\DeclareMathOperator{\var}{\mathsf{Var}}
\DeclareMathOperator{\cov}{\mathsf{Cov}}
\DeclareMathOperator{\corr}{\mathsf{Corr}}
\DeclareMathOperator{\plim}{plim}
\DeclareMathOperator{\rank}{rank}


\definecolor{ttcolor}{RGB}{0,0,1}%{RGB}{163,0,0}
\definecolor{mygray}{RGB}{248,249,250}

% Number theorem environments
\setbeamertemplate{theorem}[ams style]
\setbeamertemplate{theorems}[numbered]

% Reset theorem-like environments so that each is numbered separately
\usepackage{etoolbox}
\undef{\definition}
\theoremstyle{definition}
\newtheorem{definition}{\translate{Definition}}

% Change colours for theorem-like environments
\definecolor{mygreen1}{RGB}{0,96,0}
\definecolor{mygreen2}{RGB}{229,239,229}
\setbeamercolor{block title}{fg=white,bg=mygreen1}
\setbeamercolor{block body}{fg=black,bg=mygreen2}

\lstdefinestyle{numbers}{numbers=left, stepnumber=1, numberstyle=\tiny, numbersep=10pt}
\lstdefinestyle{MyFrame}{backgroundcolor=\color{yellow},frame=shadowbox}

\lstdefinestyle{rstyle}%
{language=R,
	basicstyle=\footnotesize\ttfamily,
	backgroundcolor = \color{mygray},
	commentstyle=\slshape\color{green!50!black},
	keywordstyle=\color{blue},
	identifierstyle=\color{blue},
	stringstyle=\color{orange},
	%escapechar=\#,
	rulecolor = \color{mygray}, 
	showstringspaces = false,
	showtabs = false,
	tabsize = 2,
	emphstyle=\color{red},
	frame = single}

\lstset{language=R,frame=single}    

\hypersetup{colorlinks, urlcolor=blue, linkcolor = myred}

\AtBeginSection{\frame{\sectionpage}}

% Remove Section 1, Section 2, etc. as titles in section pages
\defbeamertemplate{section page}{mine}[1][]{%
	\begin{centering}
		{\usebeamerfont{section name}\usebeamercolor[fg]{section name}#1}
		\vskip1em\par
		\begin{beamercolorbox}[sep=12pt,center]{part title}
			\usebeamerfont{section title}\insertsection\par
		\end{beamercolorbox}
	\end{centering}
} 

\setbeamertemplate{section page}[mine] 

\beamertemplatenavigationsymbolsempty


\title{R403: Probabilistic and Statistical Computations\\ with R}
\subtitle{Topic 14: \textcolor{myred}{Linear Regression Models in R}}
\author{Kaloyan Ganev}

\date{2022/2023} 

\begin{document}
\maketitle

\begin{frame}[fragile]
\frametitle{Lecture Contents}
\tableofcontents
\end{frame}

\section{Introduction}
\begin{frame}[fragile]
\frametitle{Introduction}
\begin{itemize}
	\item We will not repeat details related to regression analysis
	\item We only consider the basics of constructing and working with regression models in R
	\item We stick to linear models
\end{itemize}
\end{frame}

\section{Simple linear regression in R}
\begin{frame}[fragile]
\frametitle{Simple linear regression in R}
\begin{itemize}
	\item Start again with the Eurostat data on quarterly GDP and its components:
	\begin{lstlisting}[style = rstyle, breaklines]
	library(eurostat)
	library(tidyverse)
	library(xts)

	search1 <- search_eurostat("GDP", type = "dataset")

	data_gdpq <- get_eurostat("namq_10_gdp", time_format = "date")

	data_gdpq_bg <- data_gdpq %>%
		filter(geo == "BG", 
        	unit == "CLV10_MEUR",
  		    s_adj == "NSA",
        	na_item %in% c("B1GQ", "P3", "P5G", "P6", "P7")) %>%
	select(time, values, na_item) %>%
	spread(na_item, values)  %>%
	mutate(time = as.yearqtr(time))
	\end{lstlisting}
	\item Make an \textbf{xts} object from the data frame:
	\begin{lstlisting}[style = rstyle, breaklines]
	xts1 <- xts(data_gdpq_bg[,2:6],  order.by = data_gdpq_bg$time)
	\end{lstlisting}
\end{itemize} 
\end{frame}

\begin{frame}[fragile]
\frametitle{Simple linear regression in R (2)}
\begin{itemize}
	\item We will start analysing data by first visualizing them in a scatterplot
	\item \textcolor{red}{An important note:} For the time being we will deliberately allow ourselves making some mistakes related to the properties of time series data (esp. stationarity)
	\item Take aggregate consumption and GDP; the scatterplot is generated by:
	\begin{lstlisting}[style = rstyle, breaklines]
	gg1 <- ggplot(xts1) + 
		geom_point(aes(x = log(B1GQ), y = log(P3)),
             size = 4, colour = "blue", alpha = 0.5) + 
		theme_bw()
	\end{lstlisting}
	\item The graph suggests a linear relationship
	\item Run a linear regression using the \cc{lm()} command:
	\begin{lstlisting}[style = rstyle, breaklines]
	lm1 <- lm(log(P3) ~ log(B1GQ), data = xts1["2000-01/2019-04"])
	\end{lstlisting}
	\item Check with \cc{mode()} and \cc{class()} the type of the regression object
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Simple linear regression in R (3)}
\begin{itemize}
	\item To explore the regression output, the \cc{summary()} command is used:
	\begin{lstlisting}[style = rstyle, breaklines]
	print(lm1)
	summary(lm1)
	\end{lstlisting}
	\item As the model object is a list, we can also access its elements which provide additional useful information
	\item To get only the coefficients, you can try with:
	\begin{lstlisting}[style = rstyle, breaklines]
	lm1$coefficients
	\end{lstlisting}
	\item You can also do it with:
	\begin{lstlisting}[style = rstyle, breaklines]
	coef(lm1)
	\end{lstlisting}
	\item The output is a (named) vector
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Simple linear regression in R (4)}
\begin{itemize}
	\item To add the regression line to our scatterplot:
	\begin{lstlisting}[style = rstyle, breaklines]
	gg1 <- gg1 + 
  		geom_abline(intercept = lm1$coef[1], 
              		slope = lm1$coef[2],
              		size = 1.2,
              		colour = "red")
	\end{lstlisting}
	\item Using base R functionality, the same can be achieved by simply using:
	\begin{lstlisting}[style = rstyle, breaklines]
	abline(lm1)
	\end{lstlisting}
	\item Fitted values can be accessed with:
	\begin{lstlisting}[style = rstyle, breaklines]
	lm1$fitted # or
	fitted(lm1)
	\end{lstlisting}
	\item We can add the fitted values to the existing \textbf{xts} object:
	\begin{lstlisting}[style = rstyle, breaklines]
	xts1$P3_fitted <- c(rep(NA, length(xts1[,2])-length(lm1$fitted)),
                    exp(lm1$fitted))
	\end{lstlisting}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Simple linear regression in R (5)}
\begin{itemize}
	\item In order to plot together the actuals and the fitted values:
	\begin{lstlisting}[style = rstyle, breaklines]
	gg2 <- ggplot(xts1["2000/2019"], aes(x = Index)) + 
		geom_line(aes(y = P3, color = "Consumption")) + 
		geom_line(aes(y = P3_fitted, color = "Fitted"), lty = 2) + 
		xlab("") + 
		ylab("") + 
		scale_color_manual("", values = c("red", "darkgreen")) +
		ggtitle("Actual and Fitted Consumption") + 
		theme_minimal()
	\end{lstlisting}
	\item Note that the fit indicates the low quality of the regression in many respects (we will see this also in the residual plot)
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Simple linear regression in R (6)}
\begin{itemize}
	\item Residuals can be extracted with:
	\begin{lstlisting}[style = rstyle, breaklines]
	lm1$residuals # or:
	resid(lm1)
	\end{lstlisting}
	\item To add them to our \textbf{xts} object:
	\begin{lstlisting}[style = rstyle, breaklines]
	xts1$resids <- c( rep(NA, length(xts1[,2])-length(lm1$resid)), lm1$resid)
	\end{lstlisting}
	\item The residual plot:
	\begin{lstlisting}[style = rstyle, breaklines]
	gg3 <- ggplot(xts1["2000/2019"], aes(x = Index)) +
		geom_line(aes(y = resids), color = "red") + 
		xlab("") + 
		ylab("") + 
		ggtitle("Residuals") + 
		theme_minimal()
	\end{lstlisting}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Simple linear regression in R (7)}
\begin{itemize}
	\item R provides also a set of diagnostic plots directly accessible through:
	\begin{lstlisting}[style = rstyle, breaklines]
	plot(lm1)
	\end{lstlisting}
	
	\item More info on what the output is about can be found if you search for help on \cc{plot.lm()}
	
	\item Some of the output should still be already familiar to you
	
	\item What's new maybe is the \textit{scale-location} plot: almost the same as residuals vs. fitted values
	
	\item Instead of residuals, however, the square root of the absolute value of standardized residuals is used
	
	\item The plot can be considered a rough diagnostic for heteroskedasticity
	
	\item Residuals vs. leverage: leverage measures the sensitivity of the model coefficient to individual observations
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Simple linear regression in R (8)}
\begin{itemize}
	\item We can perform ANOVA on our simple model to identify sources of variation
	\begin{lstlisting}[style = rstyle, breaklines]
	anova(lm1)
	\end{lstlisting}
	\item The latter also gives us an $F$ statistic to compare nested models (one with a constant only, and one with GDP as a predictor in our case)
	\item We can get the confidence intervals for point estimates using:
	\begin{lstlisting}[style = rstyle, breaklines]
	confint(lm1, level = 0.95)
	\end{lstlisting}
	\item If you are using exactly a 95\%-confidence level, then \cc{level = 0.95} can be skipped
	\item With \cc{deviance()}, \cc{logLik()}, and \cc{AIC()} and \cc{BIC()} you get respectively the sum of squared residuals (RSS), the value of the log-likelihood\footnote{Note that it is computed under the assumption of normality of disturbances.}, and AIC and SIC
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Simple linear regression in R (9)}
\begin{itemize}
	\item There are some elements of regression output which we did not discuss
	\item To get the standard error of residuals $\displaystyle \sqrt{\frac{\sum \varepsilon_{i}^{2}}{n-k}}$:
	\begin{lstlisting}[style = rstyle, breaklines]
	sqrt(deviance(lm1)/df.residual(lm1))
	\end{lstlisting}
	\item The following:
	\begin{lstlisting}[style = rstyle, breaklines]
	df.residual(lm1)
	\end{lstlisting}
	will only get you the degrees of freedom ($n-k$)
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Simple linear regression in R (10)}
\begin{itemize}
	\item $R^{2}$ can be computed in several ways (I am skipping the hopefully familiar formulae):
	\begin{lstlisting}[style = rstyle, breaklines]
	P3_smpl <- as.numeric(log(xts1$P3)["2000/2019"])

	var(fitted(lm1))/var(P3_smpl)
	cor(P3_smpl, fitted(lm1))**2
	1 - var(resid(lm1))/var(P3_smpl)
	\end{lstlisting}
	\item Otherwise, it can be extracted from regression output in the following way:
	\begin{lstlisting}[style = rstyle, breaklines]
	smr_lm1 <- summary(lm1)
	smr_lm1$r.squared
	\end{lstlisting}
	\item Adjusted $R^{2} = \displaystyle 1 - (1-R^{2})\frac{n-1}{n-p-1}$:
	\begin{lstlisting}[style = rstyle, breaklines]
	1 - (1 - var(fitted(lm1))/var(P3_smpl)) * (length(P3_smpl)-1)/df.residual(lm1)
	smr_lm1$adj.r.squared
	\end{lstlisting}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Simple linear regression in R (11)}
\begin{itemize}
	\item To access coefficient estimates, standard errors, $t$-values, and $p$-values, you can use matrix subsetting operations
	\item For example, to get the column of standard errors:
	\begin{lstlisting}[style = rstyle, breaklines]
	coef(summary(lm1))[,2]
	\end{lstlisting}
	\item To get the $t$-statistic for consumption:
	\begin{lstlisting}[style = rstyle, breaklines]
	coef(summary(lm1))[2,3]
	\end{lstlisting}
	\item etc.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Simple linear regression in R (12)}
\begin{itemize}
	\item The $t$-statistic is simply the ratio of the coefficient estimate and its standard error: $t = \displaystyle \frac{\widehat{\beta}}{se(\widehat{\beta})}$
	\item Thus, for example:
	\begin{lstlisting}[style = rstyle, breaklines]
	t1 <- coef(summary(lm1))[1,1]/coef(summary(lm1))[1,2]
	\end{lstlisting}
	\item The corresponding $p$-value is obtained through:
	\begin{lstlisting}[style = rstyle, breaklines]
	p1 <- 2*(1 - pt(t1, df = df.residual(lm1)))
	\end{lstlisting}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Simple linear regression in R (13)}
\begin{itemize}
	\item Regression through the origin:
	\begin{lstlisting}[style = rstyle, breaklines]
	lm1a <- lm(log(P3) ~ 0 + log(B1GQ), data = xts1["2000/2019"])
	summary(lm1a)
	\end{lstlisting}
	\item Regression on a constant
	\begin{lstlisting}[style = rstyle, breaklines]
	lm1b <- lm(log(P3) ~ 1, data = xts1["2000/2019"])
	summary(lm1b)
	\end{lstlisting}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Simple linear regression in R (14)}
\begin{itemize}
	\item \cc{predict()} allows to forecast using the fitted model when new data on the independent variable are entered
	\item As an example, run the simple linear regression again but until the end of 2017 only:
	\begin{lstlisting}[style = rstyle, breaklines]
	lm1c <- lm(log(P3) ~ log(B1GQ), data = xts1["2000-01/2017-04"])
	summary(lm1c)	
	\end{lstlisting}
	
	\item Generate the prediction for 2018-2019 and then merge it to the \textbf{xts} dataset:
	\begin{lstlisting}[style = rstyle, breaklines]
	fc_lm1c <- exp(predict(lm1c, newdata = xts1["2018-01/2019-04"]))
	xts1$fc_lm1c <- c(rep(NA, length(xts1$P3)-length(fc_lm1c)), fc_lm1c)	
	\end{lstlisting}	
	
	\item This is only for demonstration as it is quite tedious
	\item There are specialized packages for time series analysis that automate a great part of the work
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Simple linear regression in R (15)}
\begin{itemize}
	\item A plot of actual, fitted, and residuals can be generated with:
	\begin{lstlisting}[style = rstyle, breaklines]
	ggplot(xts1["2000/2019"], aes(x = Index)) + 
		scale_x_continuous() +
		geom_line(aes(y = P3, color = "Consumption"))  + 
		geom_line(aes(y = P3_fitted, color = "Fitted"), lty = 2)  + 
		geom_line(aes(y = fc_lm1c, color = "Forecast"), size = 1.2)  + 
		xlab("") + 
		ylab("") + 
		scale_color_manual("", values = c("red", "darkblue", "darkgreen")) +
		ggtitle("Actual, fitted, and forecast consumption") + 
		theme_minimal()	
	\end{lstlisting}
\end{itemize}
\end{frame}

\section{Multivariate linear regression in R}
\begin{frame}[fragile]
\frametitle{Multivariate linear regression in R}
\begin{itemize}
	\item We will run a regression with three predictors (not so meaningful, just for illustration):
	\begin{lstlisting}[style = rstyle, breaklines]
	lm2 <- lm(P7 ~ P3 + P5G + P6, data = xts1)
	summary(lm2)
	\end{lstlisting}
	
	\item Add fitted values to \textbf{xts} dataset:
	\begin{lstlisting}[style = rstyle, breaklines]
	xts1$P7_fitted <- fitted(lm2)	
	\end{lstlisting}
	
	\item \ldots and plot the results:
	\begin{lstlisting}[style = rstyle, breaklines]
	ggplot(xts1["2000/2019"], aes(x = Index)) + 
		scale_x_continuous() +
		geom_line(aes(y = P7, color = "Imports"))  + 
		geom_line(aes(y = P7_fitted, color = "Fitted"), lty = 2)  + 
		xlab("") + 
		ylab("") + 
		scale_color_manual("", values = c("red", "darkblue")) +
		ggtitle("Actual and fitted imports") + 
		theme_minimal()
	\end{lstlisting}
\end{itemize}
\end{frame}

\section{Residual Tests}
\begin{frame}[fragile]
\frametitle{Normality tests: The Kolmogorov-Smirnov test}
\begin{itemize}
	\item The test is designed to compare two univariate continuous distributions
	\item It can be used to compare the distribution of an empirical variable with a pre-specified distribution, in particular the normal one
	\item Essence: Measures the \textbf{maximum} distance between the empirical distribution function ($F_{n}(x)$) and the cdf of the reference distribution ($F(x)$)
	\[
		D_{n} = \sup_{x}|F_{n}(x) - F(x)|
	\]
	\item The null hypothesis is that the data follows the reference distribution 
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Normality tests: The Kolmogorov-Smirnov test (2)}
\begin{itemize}
	\item In R, the test is implemented with the \cc{ks.test()} function available from the \textbf{stats} package
	\item Using the residuals available for the object \cc{xts1}:
	\begin{lstlisting}[style = rstyle, breaklines]
	ks.test(xts1$resids, "pnorm", mean = mean(xts1$resids, na.rm = T), sd = sd(xts1$resids, na.rm = T))	
	\end{lstlisting}
	\item We will also plot the empirical vs. the theoretical cumulative distribution:
	\begin{lstlisting}[style = rstyle, breaklines]
	ggplot(xts1["2000/2019"]) + 
		stat_ecdf(aes(x = resids), geom = "point") + 
		stat_function(fun = pnorm, 
                args = list(mean = mean(xts1$resids, na.rm = T), 
                            sd = sd(xts1$resids, na.rm = T)), 
                color = "red") + 
		xlab("") + 
		ylab("") +
		theme_minimal()
	\end{lstlisting}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Normality tests: The Jarque-Bera test}
\begin{itemize}
	\item Checks whether the sample data is characterized with kurtosis and skewness equal to the corresponding parameters of the normal distribution
	\item The test statistic is:
	\[
		JB = \frac{n}{6}\left(S^{2} + \frac{(K-3)^{2}}{4}\right)
	\]
	\item The null hypothesis is that the sample is normally distributed
	\item If the calculated value of the statistic exceeds the critical value, the hypothesis is rejected
	\item R example:
	\begin{lstlisting}[style = rstyle, breaklines]
	library(moments)
	skewness(xts1$resids, na.rm = T)
	kurtosis(xts1$resids, na.rm = T)
	
	resids_vec <- as.vector(xts1$resids["2000/2019"])
	jarque.test(resids_vec)
	\end{lstlisting}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Normality tests: Other}
\begin{itemize}
	\item Lilliefors test (some correction of K-S): \textbf{nortest} package
	\item Shapiro-Wilk test: \cc{shapiro.wilk()}
	\item Cramer-von Mises test: \textbf{nortest} package
	\item Anderson-Darling test: \textbf{nortest} package
	\item D'Agostino skewness test: \textbf{moments} package
	\item D'Agostino-Pearson omnibus test: \textbf{fBasics} package
	\item Anscombe-Glynn test: \textbf{moments} package
	\item Mardia test (Multivariate normal): \textbf{MVN} package
	\item Henze-Zirkler (Multivariate normal): \textbf{MVN} package
	\item Doornik-Hansen (Multivariate normal): \textbf{normwhn.test} package
	\item etc.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Autocorrelation tests}
\begin{itemize}
	\item Box–Pierce or Ljung–Box test statistic (from the \textbf{stats} package):
	\begin{lstlisting}[style = rstyle, breaklines]
	Box.test(xts1$resids, lag = 2, type = "Box-Pierce", fitdf = 1)
	Box.test(xts1$resids, lag = 2, type = "Ljung-Box", fitdf = 1)
	\end{lstlisting}
	\item Durbin-Watson:
	\begin{lstlisting}[style = rstyle, breaklines]
	library(lmtest)
	dwtest(lm1)	
	\end{lstlisting}
	\item Breusch-Godfrey test for higher-order autocorrelation (also requires \textbf{lmtest}):
	\begin{lstlisting}[style = rstyle, breaklines]
	bgtest(lm1, order = 4)	
	\end{lstlisting}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Heteroskedasticity tests}
\begin{itemize}
	\item Breusch-Pagan test
	\begin{lstlisting}[style = rstyle, breaklines]
	bptest(lm1)	
	\end{lstlisting}
	\item White test:
	\begin{lstlisting}[style = rstyle, breaklines]
	bptest(lm1, ~ log(B1GQ) + I(log(B1GQ)^2), data = xts1)	
	\end{lstlisting}
	\item Goldfeld-Quandt test
	\begin{lstlisting}[style = rstyle, breaklines]
	gqtest(lm1)	
	\end{lstlisting}
	\item More time will be spent on those in the forthcoming courses
\end{itemize}
\end{frame}

\end{document}